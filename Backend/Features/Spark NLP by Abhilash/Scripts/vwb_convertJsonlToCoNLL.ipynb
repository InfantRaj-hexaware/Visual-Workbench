{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "203f2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Callable, Iterator, List, Optional\n",
    "\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "from doccano_transformer import utils\n",
    "\n",
    "\n",
    "class Example:\n",
    "    def is_valid(self, raise_exception: Optional[bool] = True) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NERExample:\n",
    "\n",
    "    def __init__(self, raw: dict) -> None:\n",
    "        self.raw = raw\n",
    "        self.id = raw['id']\n",
    "        self.text = raw['text']\n",
    "        self.sentences = utils.split_sentences(raw['text'])\n",
    "        self.sentence_offsets = utils.get_offsets(raw['text'], self.sentences)\n",
    "        self.sentence_offsets.append(len(raw['text']))\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if 'entities' in self.raw:\n",
    "            labels = defaultdict(list)            \n",
    "            for annotation in self.raw['entities']:\n",
    "                annotation['user']='admin'\n",
    "                labels[annotation['user']].append([\n",
    "                    annotation['start_offset'],\n",
    "                    annotation['end_offset'],\n",
    "                    annotation['label']\n",
    "                ])\n",
    "            return labels\n",
    "        elif 'labels' in self.raw:\n",
    "            labels = defaultdict(list)\n",
    "            for label in self.raw['labels']:\n",
    "                # TODO: This format doesn't have a user field currently.\n",
    "                # So this method uses the user 0 for all label.\n",
    "                labels[0].append(label)\n",
    "            return labels\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                'The file should includes either \"labels\" or \"annotations\".'\n",
    "            )\n",
    "\n",
    "    def get_tokens_and_token_offsets(self, tokenizer):\n",
    "        tokens = [tokenizer(sentence) for sentence in self.sentences]\n",
    "        token_offsets = [\n",
    "            utils.get_offsets(sentence, tokens, offset)\n",
    "            for sentence, tokens, offset in zip(\n",
    "                self.sentences, tokens, self.sentence_offsets\n",
    "            )\n",
    "        ]\n",
    "        return tokens, token_offsets\n",
    "\n",
    "    def is_valid(self, raise_exception: Optional[bool] = True) -> bool:\n",
    "        return True\n",
    "\n",
    "    def to_conll2003(\n",
    "        self, tokenizer: Callable[[str], List[str]]\n",
    "    ) -> Iterator[dict]:\n",
    "        all_tokens, all_token_offsets = self.get_tokens_and_token_offsets(tokenizer)\n",
    "        for user, labels in self.labels.items():\n",
    "            label_split = [[] for _ in range(len(self.sentences))]\n",
    "            for label in labels:\n",
    "                for i, (start, end) in enumerate( zip(self.sentence_offsets, self.sentence_offsets[1:])):\n",
    "                    if start <= label[0] <= label[1] <= end:\n",
    "                        label_split[i].append(label)\n",
    "            lines = ['-DOCSTART- -X- -X- O\\n\\n']\n",
    "            #print(\"LABEL_SPLIT: \", label_split)\n",
    "            for tokens, offsets, label in zip(all_tokens, all_token_offsets, label_split):\n",
    "                tags = utils.create_bio_tags(tokens, offsets, label)\n",
    "\n",
    "                for token, tag in zip(tokens, tags):\n",
    "                    #print(\"TOKEN: \", token, \"TAG: \", tag)\n",
    "                    lines.append(f'{token} _ _ {tag}\\n')\n",
    "                lines.append('\\n')\n",
    "            yield {'user': user, 'data': ''.join(lines)}\n",
    "\n",
    "\n",
    "    def to_ner_conll(\n",
    "        self, tokenizer: Callable[[str], List[str]]\n",
    "    ) -> Iterator[dict]:\n",
    "        all_tokens, all_token_offsets = self.get_tokens_and_token_offsets(tokenizer)\n",
    "        for user, labels in self.labels.items():\n",
    "            label_split = [[] for _ in range(len(self.sentences))]\n",
    "            for label in labels:\n",
    "                for i, (start, end) in enumerate( zip(self.sentence_offsets, self.sentence_offsets[1:])):\n",
    "                    if start <= label[0] <= label[1] <= end:\n",
    "                        label_split[i].append(label)\n",
    "            lines = ['\\n']\n",
    "            #print(\"LABEL_SPLIT: \", label_split)\n",
    "            for tokens, offsets, label in zip(all_tokens, all_token_offsets, label_split):\n",
    "                tags = utils.create_bio_tags(tokens, offsets, label)\n",
    "\n",
    "                for token, tag in zip(tokens, tags):\n",
    "                    #print(\"TOKEN: \", token, \"TAG: \", tag)\n",
    "                    lines.append(f'{token} {tag}\\n')\n",
    "                lines.append('\\n')\n",
    "            yield {'user': user, 'data': ''.join(lines)}\n",
    "\n",
    "\n",
    "    def to_guillaume_type(\n",
    "        self, tokenizer: Callable[[str], List[str]]\n",
    "    ) -> Iterator[dict]:\n",
    "        all_tokens, all_token_offsets = self.get_tokens_and_token_offsets(tokenizer)\n",
    "        for user, labels in self.labels.items():\n",
    "            label_split = [[] for _ in range(len(self.sentences))]\n",
    "            for label in labels:\n",
    "                for i, (start, end) in enumerate( zip(self.sentence_offsets, self.sentence_offsets[1:])):\n",
    "                    if start <= label[0] <= label[1] <= end:\n",
    "                        label_split[i].append(label)\n",
    "            \n",
    "            \n",
    "            lines = []\n",
    "            labels = []\n",
    "            #print(\"LABEL_SPLIT: \", label_split)\n",
    "            \n",
    "            \n",
    "            for tokens, offsets, label in zip(all_tokens, all_token_offsets, label_split):\n",
    "                tags = utils.create_iobes_tags(tokens, offsets, label)\n",
    "\n",
    "                firstFlag = True\n",
    "                for token, tag in zip(tokens, tags):\n",
    "                    if firstFlag == True:\n",
    "                        lines.append(f'{token}')\n",
    "                        labels.append(f'{tag}')\n",
    "                        firstFlag = False\n",
    "                    else:\n",
    "                        lines.append(f' {token}')\n",
    "                        labels.append(f' {tag}')\n",
    "            yield {'user': user, 'tokens': ''.join(lines), 'labels': ''.join(labels)}\n",
    "\n",
    "    def to_spacy(\n",
    "        self, tokenizer: Callable[[str], List[str]]\n",
    "    ) -> Iterator[dict]:\n",
    "        all_tokens, all_token_offsets = self.get_tokens_and_token_offsets(\n",
    "            tokenizer)\n",
    "        for user, labels in self.labels.items():\n",
    "            label_split = [[] for _ in range(len(self.sentences))]\n",
    "            for label in labels:\n",
    "                for i, (start, end) in enumerate(\n",
    "                        zip(self.sentence_offsets, self.sentence_offsets[1:])):\n",
    "                    if start <= label[0] <= label[1] <= end:\n",
    "                        label_split[i].append(label)\n",
    "\n",
    "            data = {'raw': self.text}\n",
    "            sentences = []\n",
    "            for tokens, offsets, label in zip(\n",
    "                    all_tokens, all_token_offsets, label_split):\n",
    "                tokens = utils.convert_tokens_and_offsets_to_spacy_tokens(\n",
    "                    tokens, offsets\n",
    "                )\n",
    "                tags = biluo_tags_from_offsets(tokens, label)\n",
    "                tokens_for_spacy = []\n",
    "                for i, (token, tag, offset) in enumerate(\n",
    "                    zip(tokens, tags, offsets)\n",
    "                ):\n",
    "                    tokens_for_spacy.append(\n",
    "                        {'id': i, 'orth': str(token), 'ner': tag}\n",
    "                    )\n",
    "                sentences.append({'tokens': tokens_for_spacy})\n",
    "            data['sentences'] = sentences\n",
    "            yield {'user': user, 'data': {'id': self.id, 'paragraphs': [data]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a091869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets, biluo_tags_to_spans\n",
    "from doccano_transformer.datasets import NERDataset\n",
    "from doccano_transformer.utils import read_jsonl\n",
    "\n",
    "dataset = read_jsonl(filepath='JsonL/admin_7.jsonl', dataset=NERDataset, encoding='utf-8')\n",
    "items = dataset.to_conll2003(tokenizer=str.split)\n",
    "\n",
    "with open(\"JsonL/test.dataset\", \"w\", encoding = \"utf-8\") as file:\n",
    "    for entry in items:\n",
    "        file.write(entry[\"data\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0acf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1d8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
